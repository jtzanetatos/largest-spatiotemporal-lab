# config/trainer/callbacks/base.yaml

model_checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: "val_loss"
  mode: "min"
  save_top_k: 1
  filename: "${model.name}-{epoch:02d}-{val_loss:.4f}"
  dirpath: ${paths.checkpoints_dir}

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: "val_loss"
  mode: "min"
  patience: 10

lr_monitor:
  _target_: pytorch_lightning.callbacks.LearningRateMonitor
  logging_interval: "step"